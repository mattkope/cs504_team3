{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-03T04:48:42.501571Z","iopub.execute_input":"2024-12-03T04:48:42.502636Z","iopub.status.idle":"2024-12-03T04:48:43.766157Z","shell.execute_reply.started":"2024-12-03T04:48:42.502584Z","shell.execute_reply":"2024-12-03T04:48:43.764676Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cleaned-ecommerce-data/2019-Dec-Cleaned.csv\n/kaggle/input/cleaned-ecommerce-data/2019-Nov-Cleaned.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# File path for the December dataset\ndecember_file = \"/kaggle/input/cleaned-ecommerce-data/2019-Dec-Cleaned.csv\"\n\n# Load the December dataset\ndecember_df = pd.read_csv(december_file)\n\n# Display December data preview\nprint(\"December Data Preview:\")\nprint(december_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T04:49:31.387743Z","iopub.execute_input":"2024-12-03T04:49:31.388302Z","iopub.status.idle":"2024-12-03T04:49:41.065018Z","shell.execute_reply.started":"2024-12-03T04:49:31.388263Z","shell.execute_reply":"2024-12-03T04:49:41.063738Z"}},"outputs":[{"name":"stdout","text":"December Data Preview:\n                event_time event_type  product_id          category_id  \\\n0  2019-12-01 00:00:00 UTC       view     1005105  2232732093077520756   \n1  2019-12-01 00:00:01 UTC       view     2402273  2232732100769874463   \n2  2019-12-01 00:00:02 UTC       view    20100164  2232732110089618156   \n3  2019-12-01 00:00:02 UTC       view   100008256  2053013561185141473   \n4  2019-12-01 00:00:03 UTC       view     1005239  2232732093077520756   \n\n                  category_code   brand    price    user_id  \\\n0      construction.tools.light   apple  1302.48  556695836   \n1  appliances.personal.massager   bosch   313.52  539453785   \n2              apparel.trousers    nika   101.68  517987650   \n3          accessories.umbrella    ikea   163.56  542860793   \n4      construction.tools.light  xiaomi   256.38  525740700   \n\n                           user_session  \n0  ca5eefc5-11f9-450c-91ed-380285a0bc80  \n1  5ee185a7-0689-4a33-923d-ba0130929a76  \n2  906c6ca8-ff5c-419a-bde9-967ba8e2233e  \n3  a1bcb550-1065-4769-a80a-0ccb4bcee78d  \n4  370e8c88-3d07-41df-9aaa-2adf5a0bf312  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Feature 1: Total Spent\ndecember_df[\"total_spent\"] = december_df[\"price\"]\n\n# Feature 2: Extract Date-Time Features\ndecember_df[\"event_time\"] = pd.to_datetime(december_df[\"event_time\"])\ndecember_df[\"hour\"] = december_df[\"event_time\"].dt.hour\ndecember_df[\"day\"] = december_df[\"event_time\"].dt.day\ndecember_df[\"month\"] = december_df[\"event_time\"].dt.month\n\n# Feature 3: Aggregate User Data\nuser_agg_dec = december_df.groupby(\"user_id\").agg(\n    avg_spent_per_user=(\"total_spent\", \"mean\"),\n    num_sessions=(\"user_session\", \"nunique\")  # Count unique sessions per user\n).reset_index()\n\n# Save the processed features\nuser_agg_dec.to_csv(\"/kaggle/working/december_processed_data.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T04:51:33.998446Z","iopub.execute_input":"2024-12-03T04:51:33.998969Z","iopub.status.idle":"2024-12-03T04:51:38.773210Z","shell.execute_reply.started":"2024-12-03T04:51:33.998925Z","shell.execute_reply":"2024-12-03T04:51:38.772028Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# XGBoost Section\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Add dummy 'target' column for demonstration (replace with actual target column if available)\nimport numpy as np\nuser_agg_dec[\"target\"] = np.random.choice([0, 1], size=len(user_agg_dec))\n\n# Split features (X) and target (y)\nX = user_agg_dec.drop([\"user_id\", \"target\"], axis=1)\ny = user_agg_dec[\"target\"]\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train XGBoost model\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = model.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T04:51:42.187166Z","iopub.execute_input":"2024-12-03T04:51:42.187628Z","iopub.status.idle":"2024-12-03T04:51:44.199220Z","shell.execute_reply.started":"2024-12-03T04:51:42.187590Z","shell.execute_reply":"2024-12-03T04:51:44.197899Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.49910312213845787\n","output_type":"stream"}],"execution_count":4}]}