{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9918860,"sourceType":"datasetVersion","datasetId":6090618}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-02T23:53:30.637934Z","iopub.execute_input":"2024-12-02T23:53:30.638410Z","iopub.status.idle":"2024-12-02T23:53:31.937597Z","shell.execute_reply.started":"2024-12-02T23:53:30.638371Z","shell.execute_reply":"2024-12-02T23:53:31.936290Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cleaned-ecommerce-data/2019-Dec-Cleaned.csv\n/kaggle/input/cleaned-ecommerce-data/2019-Nov-Cleaned.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\n\n# Get the input file paths dynamically\nfile_paths = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        file_paths.append(os.path.join(dirname, filename))\n\n# Combine data from all CSV files in the directory\ndataframes = [pd.read_csv(file) for file in file_paths if file.endswith(\".csv\")]\ncombined_df = pd.concat(dataframes, ignore_index=True)\n\n# Display the first few rows of the combined data\nprint(\"Combined Data Preview:\")\nprint(combined_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T01:13:52.551099Z","iopub.execute_input":"2024-12-03T01:13:52.551537Z","iopub.status.idle":"2024-12-03T01:16:37.448102Z","shell.execute_reply.started":"2024-12-03T01:13:52.551492Z","shell.execute_reply":"2024-12-03T01:16:37.446346Z"}},"outputs":[{"name":"stdout","text":"Combined Data Preview:\n                event_time event_type  product_id          category_id  \\\n0  2019-12-01 00:00:00 UTC       view     1005105  2232732093077520756   \n1  2019-12-01 00:00:01 UTC       view     2402273  2232732100769874463   \n2  2019-12-01 00:00:02 UTC       view    20100164  2232732110089618156   \n3  2019-12-01 00:00:02 UTC       view   100008256  2053013561185141473   \n4  2019-12-01 00:00:03 UTC       view     1005239  2232732093077520756   \n\n                  category_code   brand    price    user_id  \\\n0      construction.tools.light   apple  1302.48  556695836   \n1  appliances.personal.massager   bosch   313.52  539453785   \n2              apparel.trousers    nika   101.68  517987650   \n3          accessories.umbrella    ikea   163.56  542860793   \n4      construction.tools.light  xiaomi   256.38  525740700   \n\n                           user_session  \n0  ca5eefc5-11f9-450c-91ed-380285a0bc80  \n1  5ee185a7-0689-4a33-923d-ba0130929a76  \n2  906c6ca8-ff5c-419a-bde9-967ba8e2233e  \n3  a1bcb550-1065-4769-a80a-0ccb4bcee78d  \n4  370e8c88-3d07-41df-9aaa-2adf5a0bf312  \n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Inspect column names\nprint(\"Column Names:\")\nprint(combined_df.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T01:33:55.713782Z","iopub.execute_input":"2024-12-03T01:33:55.714505Z","iopub.status.idle":"2024-12-03T01:33:55.725592Z","shell.execute_reply.started":"2024-12-03T01:33:55.714441Z","shell.execute_reply":"2024-12-03T01:33:55.724037Z"}},"outputs":[{"name":"stdout","text":"Column Names:\nIndex(['event_time', 'event_type', 'product_id', 'category_id',\n       'category_code', 'brand', 'price', 'user_id', 'user_session'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Feature 1: Total Spent (ensure 'price' exists and is numeric)\n# Assuming we multiply 'price' by an implicit quantity of 1 (if no quantity column exists)\ncombined_df[\"total_spent\"] = combined_df[\"price\"]\n\n# Feature 2: Extract Date-Time Features (ensure 'event_time' is the timestamp)\ncombined_df[\"event_time\"] = pd.to_datetime(combined_df[\"event_time\"])\ncombined_df[\"hour\"] = combined_df[\"event_time\"].dt.hour\ncombined_df[\"day\"] = combined_df[\"event_time\"].dt.day\ncombined_df[\"month\"] = combined_df[\"event_time\"].dt.month\n\n# Feature 3: Aggregate User Data\nuser_agg = combined_df.groupby(\"user_id\").agg(\n    avg_spent_per_user=(\"total_spent\", \"mean\"),\n    num_sessions=(\"user_session\", \"nunique\")  # Example: count unique sessions per user\n).reset_index()\n\n# Save the processed features\nuser_agg.to_csv(\"/kaggle/working/processed_data.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T01:39:56.257348Z","iopub.execute_input":"2024-12-03T01:39:56.257774Z","iopub.status.idle":"2024-12-03T01:40:47.853180Z","shell.execute_reply.started":"2024-12-03T01:39:56.257735Z","shell.execute_reply":"2024-12-03T01:40:47.851676Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# XGBoost Section\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T01:41:42.074759Z","iopub.execute_input":"2024-12-03T01:41:42.075256Z","iopub.status.idle":"2024-12-03T01:41:42.081303Z","shell.execute_reply.started":"2024-12-03T01:41:42.075212Z","shell.execute_reply":"2024-12-03T01:41:42.079645Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import numpy as np\n\n# Generate a binary target column with random 0s and 1s\nuser_agg[\"target\"] = np.random.choice([0, 1], size=len(user_agg))\n\n# Split features (X) and target (y)\nX = user_agg.drop([\"user_id\", \"target\"], axis=1)\ny = user_agg[\"target\"]\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train XGBoost model\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = model.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T01:44:03.047276Z","iopub.execute_input":"2024-12-03T01:44:03.047760Z","iopub.status.idle":"2024-12-03T01:44:12.699490Z","shell.execute_reply.started":"2024-12-03T01:44:03.047715Z","shell.execute_reply":"2024-12-03T01:44:12.697922Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.5002540869807824\n","output_type":"stream"}],"execution_count":17}]}